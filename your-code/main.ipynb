{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"kg_train.csv\") # avec underscore\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 1) (200, 1)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X = colonnes de features, y = colonne cible\n",
    "X = data.iloc[:, :-1]  # toutes sauf la dernière\n",
    "y = data.iloc[:, -1]   # dernière colonne\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import re\n",
    "\n",
    "def clean_html(raw_html):\n",
    "    # 1. Supprimer JavaScript/CSS\n",
    "    no_js_css = re.sub(r\"(?is)<(script|style).*?>.*?(</\\1>)\", \"\", raw_html)\n",
    "\n",
    "    # 2. Supprimer les commentaires HTML\n",
    "    no_comments = re.sub(r\"<!--(.*?)-->\", \"\", no_js_css)\n",
    "\n",
    "    # 3. Supprimer toutes les balises HTML\n",
    "    clean_text = re.sub(r\"<.*?>\", \"\", no_comments)\n",
    "\n",
    "    return clean_text.strip()\n",
    "\n",
    "# Exemple d'utilisation\n",
    "html_code = \"\"\"\n",
    "<html>\n",
    "<head><style>body {color: red;}</style></head>\n",
    "<body>\n",
    "<!-- Ceci est un commentaire -->\n",
    "<p>Hello <b>World</b>!</p>\n",
    "<script>alert('test');</script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "print(clean_html(html_code))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello this is example with some numbers and special chars\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\W', ' ', text)          # Remove special characters\n",
    "    text = re.sub(r'\\d', '', text)            # Remove numbers\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)  # Remove single characters\n",
    "    text = re.sub(r'^[a-zA-Z]\\s+', '', text)  # Remove single char from start\n",
    "    text = re.sub(r'\\s+', ' ', text)          # Replace multiple spaces\n",
    "    text = re.sub(r'^b\\s+', '', text)         # Remove prefix b\n",
    "    text = text.lower()                       # Lowercase\n",
    "    return text.strip()\n",
    "\n",
    "# Exemple\n",
    "sample = \"Hello!!! This is b Example 123, with some numbers 45 and special @#$ chars.\"\n",
    "print(clean_text(sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example showing stopwords removal.\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Charger la liste des stopwords anglais\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Exemple\n",
    "sample_text = \"This is an example showing off stopwords removal.\"\n",
    "print(remove_stopwords(sample_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\katyd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\katyd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\katyd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original : running runs ran easily fairly\n",
      "Après lemmatisation\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Télécharger les ressources nécessaires\n",
    "nltk.download('punkt')      # Tokenizer\n",
    "nltk.download('wordnet')    # WordNet data\n",
    "nltk.download('omw-1.4')    # WordNet mapping (obligatoire dans certaines versions)\n",
    "\n",
    "# Initialiser le lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Exemple de texte\n",
    "sample_text = \"running runs ran easily fairly\"\n",
    "\n",
    "# Tokenisation en mots\n",
    "words = nltk.word_tokenize(sample_text)\n",
    "\n",
    "# Appliquer la lemmatisation\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(\"Texte original :\", sample_text)\n",
    "print(\"Après lemmatisation\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Ham Words:\n",
      "Empty DataFrame\n",
      "Columns: [word, count]\n",
      "Index: []\n",
      "\n",
      "Top Spam Words:\n",
      "Empty DataFrame\n",
      "Columns: [word, count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "# Exemple : suppose que ton DataFrame s'appelle `data` avec colonnes 'label' et 'message'\n",
    "# label = 'ham' ou 'spam'\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def top_words(df, label, n=10, language='english'):\n",
    "    # Nettoyage de base\n",
    "    df = df.copy()\n",
    "    df['label'] = df['label'].astype(str).str.strip().str.lower()\n",
    "    df['text']  = df['text'].astype(str).fillna('').str.strip()\n",
    "\n",
    "    # Filtrer sur le label\n",
    "    subset = df.loc[df['label'] == label, 'text']\n",
    "    # Retirer lignes vides\n",
    "    subset = subset[subset.str.len() > 0]\n",
    "\n",
    "    if subset.empty:\n",
    "        return pd.DataFrame({'word': [], 'count': []})  # évite le crash\n",
    "\n",
    "    # Vectorizer (choisis la langue de stopwords selon tes données)\n",
    "    stop_lang = language  # 'english' ou \n",
    "    vectorizer = CountVectorizer(\n",
    "        stop_words=stop_lang,\n",
    "        lowercase=True,\n",
    "        token_pattern=r'(?u)\\b\\w\\w+\\b',  # mots de 2+ caractères\n",
    "        min_df=1\n",
    "    )\n",
    "\n",
    "    bow = vectorizer.fit_transform(subset.tolist())\n",
    "    if bow.shape[1] == 0:\n",
    "        return pd.DataFrame({'word': [], 'count': []})\n",
    "\n",
    "    counts = bow.sum(axis=0).A1\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    freq = pd.DataFrame({'word': words, 'count': counts}).sort_values('count', ascending=False)\n",
    "    return freq.head(n)\n",
    "\n",
    "\n",
    "# Top 10 mots pour ham et spam\n",
    "print(\"Top Ham Words:\")\n",
    "print(top_words(data, 'ham', 10, language='english'))   \n",
    "print(\"\\nTop Spam Words:\")\n",
    "print(top_words(data, 'spam', 10, language='english'))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "# Séparation train / validation\n",
    "import re, unicodedata, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1) Preprocess \"text\" into \"preprocessed text\" ---\n",
    "def preprocess_text(x):\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    t = str(x).lower()\n",
    "    t = re.sub(r\"http\\S+|www\\.\\S+\", \" \", t)      # URLs\n",
    "    t = re.sub(r\"\\S+@\\S+\", \" \", t)               # emails\n",
    "    t = re.sub(r\"[@#]\\w+\", \" \", t)               # @mentions / #hashtags\n",
    "    t = re.sub(r\"\\d+\", \" \", t)                   # numbers\n",
    "    # Remove accents (comment out next 2 lines if you want to keep accents)\n",
    "    t = unicodedata.normalize(\"NFKD\", t)\n",
    "    t = t.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    t = re.sub(r\"[^\\w\\s]\", \" \", t)               # punctuation/special chars\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()           # collapse whitespace\n",
    "    return t\n",
    "\n",
    "data[\"preprocessed text\"] = data[\"text\"].map(preprocess_text)\n",
    "\n",
    "# --- 2) Train/val split ---\n",
    "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 3) Your feature creation (now safe) ---\n",
    "for df in [data_train, data_val]:\n",
    "    s = df[\"preprocessed text\"]\n",
    "    df[\"money_mark\"]       = s.str.contains(money_symbol_list, case=False, regex=True).astype(int)\n",
    "    df[\"suspicious_words\"] = s.str.contains(suspicious_words,   case=False, regex=True).astype(int)\n",
    "    df[\"text_len\"]         = s.str.len()\n",
    "\n",
    "# Quick sanity check\n",
    "# print(data_train[[\"preprocessed text\",\"money_mark\",\"suspicious_words\",\"text_len\"]].head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    data[\"preprocessed text\"], data[\"label\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Vectorize + classify in one pipeline\n",
    "model = make_pipeline(\n",
    "    CountVectorizer(\n",
    "        lowercase=True,\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=(1,2),      # unigrams + bigrams\n",
    "        min_df=2                # ignore very rare terms (optional)\n",
    "    ),\n",
    "    LogisticRegression(max_iter=1000)\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Validation accuracy:\", model.score(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (1000, 30348)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Example: assuming you have a pandas DataFrame `data` with a column 'preprocessed text'\n",
    "corpus = data[\"preprocessed text\"]  # replace with your text column\n",
    "\n",
    "# 1. Load the vectorizer\n",
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words=\"english\")\n",
    "\n",
    "# 2. Vectorize the whole dataset\n",
    "X_tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# 3. Print the shape\n",
    "print(\"TF-IDF matrix shape:\", X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.99\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       112\n",
      "           1       1.00      0.98      0.99        88\n",
      "\n",
      "    accuracy                           0.99       200\n",
      "   macro avg       0.99      0.99      0.99       200\n",
      "weighted avg       0.99      0.99      0.99       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Use your text column\n",
    "TEXT_COL = \"preprocessed text\"  # change to \"text\" if that's what you have\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    data[TEXT_COL], data[\"label\"],\n",
    "    test_size=0.2, random_state=42, stratify=data[\"label\"]\n",
    ")\n",
    "\n",
    "# TF-IDF + Logistic Regression classifier\n",
    "clf = make_pipeline(\n",
    "    TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=(1,2),   # unigrams + bigrams\n",
    "        max_features=50000,  # cap vocab size (optional)\n",
    "    ),\n",
    "    LogisticRegression(max_iter=1000, n_jobs=-1)  # strong baseline\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "pred = clf.predict(X_val)\n",
    "print(\"Validation accuracy:\", accuracy_score(y_val, pred))\n",
    "print(classification_report(y_val, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "mon_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
